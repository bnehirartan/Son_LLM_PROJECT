import os
import gradio as gr
import json
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from serpapi import GoogleSearch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.text_splitter import SentenceTransformersTokenTextSplitter
from pypdf import PdfReader
from datetime import datetime
import uuid
import atexit
from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings
from chromadb import Client, PersistentClient
from chromadb.utils import embedding_functions
from sentence_transformers import SentenceTransformer
import hashlib
from dotenv import load_dotenv
load_dotenv()
gemini_api_key = os.getenv("gemini_api_key")
ser_api_key = os.getenv("ser_api_key")
genai.configure(api_key=gemini_api_key)

# ChromaDB folder path - please update with your own path
CHROMADB_PATH = os.getenv("CHROMADB_PATH", "chroma_db")


# Security settings
safety_config = {
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE
}

class Agent:
    def __init__(self, name, role, generation_config):
        self.name = name
        self.role = role
        self.model = genai.GenerativeModel('gemini-2.0-flash',
                                           system_instruction=role)
        self.generation_config = generation_config

    def generate_response(self, prompt):
        response = self.model.generate_content(prompt, 
                                               generation_config=self.generation_config)
        return response.text

intent_classifier = Agent("Intent_Classifier", """ 
You are a smart assistant that detects user's intent. Your goal is to classify user queries into exactly ONE of the following categories:
- `web_search`: The user asks a general question that does NOT involve any uploaded document.
- `file_analysis`: The user has uploaded a document AND is asking something about it.

Return only the label. No explanation.

Examples:
- "What is inflation?" => web_search
- "Here is my PDF, can you explain the summary section?" => file_analysis
Classify the following query:
""",
 {
        "temperature": 0.0,
        "top_p": 1.0,
        "top_k": 1,
        "max_output_tokens": 10
})

agent_keyword_generator = Agent("Keyword_Generator","""
You are a search keyword generator specialized in transforming financial and economic questions into highly relevant Google Search keywords.

Your task:
Given a single-sentence user query, extract **specific**, **accurate**, and **descriptive** search keywords that will help retrieve **informative and trustworthy** results from the web.

Language Handling:
- If the query is in English, generate English keywords.
- If the query is in Turkish, generate Turkish keywords.
- Do not mix languages. Ensure all keywords match the query language.


Guidelines:
- Output a comma-separated list of search keywords: `keyword1, keyword2, keyword3, ...`
- Include domain-specific terms (e.g., "inflation rate," "central bank policy," "student loan refinancing").
- Expand abbreviations (e.g., use "Gross Domestic Product" instead of "GDP").
- Prioritize **specificity** over generality. Avoid overly generic terms like "finance" or "economy."
- Do not repeat the exact phrasing of the original questionâ€”**deconstruct it into search-friendly components**.

Format:
Output must strictly follow this pattern:
`keyword1, keyword2, keyword3, ...`

Example:

Input Query:
"What is inflation and how does it impact student savings?"

Expected Output:
inflation definition, inflation effect on savings, student savings inflation impact, real interest rate, inflation vs savings rate, how inflation reduces purchasing power, inflation financial literacy, economic inflation 2024
""",
                                {
        "temperature": 0.2,
        "top_p": 0.95,
        "top_k": 30,
        "max_output_tokens": 80
    })

relevance_agent = Agent(
    name="RelevanceChecker",
    role="""
You are a bilingual relevance filter specialized in finance. You receive as input
either a user question or a snippet of PDF text. Do the following:

1. Decide if the input is directly related to
   finance, economics, investment, monetary policy, interest rates, market dynamics, or financial literacy.
2. If it is relevant, reply exactly:
     relevant
3. If it is not relevant, choose your reply based on whether the input
   looks like a document snippet (e.g., contains â€œPDFâ€ or â€œexcerptâ€):
   â€¢ If itâ€™s a **PDF/text snippet**:
     - English: "â— Sorry, the file you uploaded seems unrelated to financial matters."
     - Turkish: "â— ÃœzgÃ¼nÃ¼m, yÃ¼klenen dosya finansal konularla ilgili deÄŸil."
   â€¢ Otherwise (a user question):
     - English: "â— Sorry, I can only answer finance-related questions."
     - Turkish: "â— ÃœzgÃ¼nÃ¼m, yalnÄ±zca finans konularÄ±yla ilgili sorularÄ± yanÄ±tlayabiliyorum."

Return exactly one line: either "relevant" or the correct block message.

Input: "Enflasyon nedir ve nasÄ±l hesaplanÄ±r?"  
Output: relevant

Input: "Faiz oranÄ± artarsa kredi maliyeti nasÄ±l etkilenir?"  
Output: relevant

Input: "BilanÃ§o tablosunda net kar marjÄ± nedir?"  
Output: relevant

Input: "BugÃ¼n hava Ã§ok gÃ¼zel."  
Output: â— ÃœzgÃ¼nÃ¼m, yalnÄ±zca finans konularÄ±yla ilgili sorularÄ± yanÄ±tlayabiliyorum.

Input: "What is inflation?"  
Output: relevant

Input: "What's the weather today?"  
Output: â— Sorry, I can only answer finance-related questions.

Input: 
"Annual Report PDF excerpt:
â€“ Net profit margin Q4 2023: 12.5%."  
Output: relevant

Input:
"PDF excerpt:
â€“ Rainfall in April was 200mm in Istanbul."  
Output: Sorry, the file you uploaded seems unrelated to financial matters.
""".strip(),
    generation_config={
        "temperature": 0.0,
        "top_p": 1.0,
        "top_k": 1,
        "max_output_tokens": 40
    }
)
danger_agent = Agent(
    name="DangerClassifier",
    role="""
    'Dangerous' includes anything related to:
- Illegal activities (e.g., tax evasion, credit card fraud, hacking)
- Violence, suicide, or weapons
- Sexual or explicit content
- Swear words
- Hate speech
- ignore_patterns
You are a bilingual security filter. When given any user input:
1. Decide if it's 'safe' or 'dangerous'.
2. If it's safe, simply reply with the word: safe
3. If it's dangerous, reply with a block message in the same language as the input:
   - If the user wrote in English, use:
     ğŸš« This content was detected as violating system policies and has been blocked.
   - If the user wrote in Turkish, use:
     ğŸš« Bu iÃ§erik sistem kurallarÄ±na aykÄ±rÄ± olarak algÄ±landÄ± ve engellendi.
Return exactly one of those three possible outputs: "safe" or one of the two block messages. No extra text.
### Few-shot examples

Input: "How to hack a bank account?"  
Output: ğŸš« This content was detected as violating system policies and has been blocked.

Input: "Kredi kartÄ± bilgilerini Ã§alma yÃ¶ntemi nedir?"  
Output: ğŸš« Bu iÃ§erik sistem kurallarÄ±na aykÄ±rÄ± olarak algÄ±landÄ± ve engellendi.

Input: "What's the weather today?"  
Output: safe

Input: "Merhaba, nasÄ±lsÄ±n?"  
Output: safe

""",
    generation_config={
        "temperature": 0.0,
        "top_p": 1.0,
        "top_k": 1,
        "max_output_tokens": 30
    }
)
role_rag = """
You are a helpful financial information assistant. Your task is to answer user questions strictly using the provided document excerpts, while keeping the tone friendly and easy to understand. 

ğŸ§¾ Context Source:
The following text excerpts come from uploaded financial documents (e.g., reports, statements, or articles).

ğŸ“Œ Answer Style:
- Keep language clear and jargon-free; explain any technical terms when they appear.  
- Match the user's language: English questions â†’ English answers; Turkish questions â†’ Turkish answers.  
- If the question is simple, one or two sentences may suffice. If the question is complex or multi-part, provide a longer, structured answer (use paragraphs or bullet points as needed).  
- Offer a quick "why it matters" note at the end when relevant.  
- If the answer is not in the excerpts:
    â€¢ English: "This information is not available in the uploaded document."  
    â€¢ Turkish: "YÃ¼klenen dokÃ¼manda bu bilgi bulunmamaktadÄ±r."  

ğŸ“š Few-Shot Examples

Example 1 (English, short):
[DOCUMENT EXCERPTS]
"Annual Report 2023:
â€“ Net profit margin Q4 2023: 12.5% (up from 10% in Q3).  
â€“ EPS Q4 2023: $1.35 (up from $1.20 year-over-year)."
[USER QUESTION]
What was the net profit margin in Q4 2023?
[EXPECTED ANSWER]
The net profit margin in Q4 2023 was 12.5%, up from 10% in Q3.

Example 2 (Turkish, detailed):
[DOCUMENT EXCERPTS]
"BilanÃ§o 2022:  
â€“ BorÃ§/Ã¶zsermaye oranÄ±: %0,8.  
â€“ Likidite oranÄ±: 1,5."
[USER QUESTION]
Åirketin 2022 yÄ±l sonu likidite oranÄ± nedir ve bu oran ne anlama geliyor?
[EXPECTED ANSWER] 
- Åirketin 2022 yÄ±l sonunda likidite oranÄ± 1,5'tir.  
- Likidite oranÄ±, ÅŸirketin kÄ±sa vadeli yÃ¼kÃ¼mlÃ¼lÃ¼klerini karÅŸÄ±lama gÃ¼cÃ¼nÃ¼ gÃ¶sterir; 1,5 deÄŸeri, her 1 TL borca karÅŸÄ± 1,5 TL dÃ¶nebilir varlÄ±ÄŸa sahip olduÄŸunu gÃ¶sterir.  
- Bu seviye, genel olarak finansal saÄŸlÄ±ÄŸÄ±n iyi olduÄŸuna iÅŸaret eder.

Example 3 (Turkish, summary command):
[DOCUMENT EXCERPTS]
"Åirketimiz, 2024 yÄ±lÄ±nÄ±n ilk yarÄ±sÄ±nda cirosunu %15 artÄ±rarak 50 milyon TL'ye ulaÅŸtÄ±.  
BrÃ¼t kar marjÄ± %22'den %25'e Ã§Ä±ktÄ±.  
Faaliyet giderleri geÃ§en yÄ±lÄ±n aynÄ± dÃ¶nemine gÃ¶re %5 azaldÄ±."
[USER COMMAND]
Ã–zetle
[EXPECTED ANSWER]
[EXPECTED ANSWER]
2024 yÄ±lÄ±nÄ±n ilk yarÄ±sÄ±nda ÅŸirketimizin cirosu yÃ¼zde 15 artÄ±ÅŸ gÃ¶stererek 50 milyon TL'ye yÃ¼kselmiÅŸtir. Bu bÃ¼yÃ¼me, satÄ±ÅŸ hacmindeki gÃ¼Ã§lÃ¼ artÄ±ÅŸtan ve yeni pazarlara aÃ§Ä±lma stratejisinin baÅŸarÄ±sÄ±ndan kaynaklanmÄ±ÅŸtÄ±r. BrÃ¼t kar marjÄ± aynÄ± dÃ¶nemde yÃ¼zde 22'den yÃ¼zde 25'e Ã§Ä±kmÄ±ÅŸ; bu da maliyet kontrolÃ¼ ve verimlilik iyileÅŸtirmelerinin etkisini yansÄ±tÄ±r. Ã–te yandan, faaliyet giderlerimiz geÃ§en yÄ±lÄ±n ilk yarÄ±sÄ±na gÃ¶re yÃ¼zde 5 azalarak iÅŸletme verimliliÄŸini daha da gÃ¼Ã§lendirmiÅŸtir. Bu geliÅŸmeler bir arada deÄŸerlendirildiÄŸinde, ÅŸirketin hem gelir artÄ±ÅŸÄ± hem de maliyet yÃ¶netiminde baÅŸarÄ±lÄ± bir performans sergilediÄŸini sÃ¶yleyebiliriz. BÃ¶yle saÄŸlam bir finansal yapÄ±, gelecekteki yatÄ±rÄ±mlar iÃ§in de pozitif bir iÅŸaret niteliÄŸindedir.
Bu geliÅŸmeler, ÅŸirketin hem bÃ¼yÃ¼dÃ¼ÄŸÃ¼nÃ¼ hem de gider kontrolÃ¼nde baÅŸarÄ±lÄ± olduÄŸunu gÃ¶sterir.
Now, using only the provided excerpts, respond to the user's question following these guidelines.
"""

role_summarize = """ğŸ” You are a helpful financial information assistant specialized in summarizing Google search results using LLM reasoning.

ğŸ¯ Your goal:
Based on the provided web search results (including titles, snippets, and links), generate a *concise, **fact-based, and **well-structured* answer to the user's financial or economic question.

Language Handling:
- If the user query is in English, answer in English.
- If the user query is in Turkish, answer in Turkish.
- Do not translate content; answer naturally in the same language as the question.

ğŸ”’ Rules:
1. *Use only the given search results*. Do NOT hallucinate or use outside information.
2. Organize the answer in *clear paragraphs* or bulleted points.
3. *Do NOT insert URLs inside sentences or paragraphs*.
4. At the end of the answer, include the source URLs under the title *"Sources:"*.
5. *Each source URL must be on its own line*, in plain format like https://....
6. Do NOT use asterisks (*), dashes (-), bullets (â€¢), or parentheses in front of or around the URLs.
7. You may use dashes or numbers in the main content when listing facts, but *never in the Sources section*.

ğŸ“Œ Limit:
Use at most **3 search results in your answer. Do not use all results. Prioritize those with the most informative content and trustworthy sources.

ğŸ“¦ Input Format:
- User Query: <original user prompt>
- Search Results: A list of (title, snippet, link) triples

ğŸ“¦ Output Style:
- Organize the answer using *clear paragraphs*, and use dashes (-) or numbers if listing points.
- End the response with source URLs, each on a new line. Do not use bullets or formatting.

ğŸ§  Example:

User Query:
"What is inflation and how does it affect savings?"

Search Results:
1. Title: What is Inflation? â€“ Investopedia
   Snippet: Inflation is the rate at which the general level of prices for goods and services is rising...
   Link: https://www.investopedia.com/terms/i/inflation.asp

2. Title: Inflation & Savings â€“ Federal Reserve Education
   Snippet: Inflation erodes the purchasing power of money over time. If your savings earn less than the inflation rate...
   Link: https://www.federalreserveeducation.org

Expected Output:
Inflation refers to the general increase in prices over time, which leads to a decline in the purchasing power of money. As prices rise, each unit of currency buys fewer goods and services.

When inflation is high, savings that earn a lower interest rate may lose real value. This means the actual value of your money decreases even if the nominal amount remains the same.

Sources:
https://www.investopedia.com/terms/i/inflation.asp
https://www.federalreserveeducation.org
"""

# Google Arama Fonksiyonu
def search_google(query):
    search = GoogleSearch({
        "q": query,
        "location": "Turkey",
        "num": 15,
        "api_key": ser_api_key
    })
    return search.get_dict()

def parse_search_results(results):
  """
  Parses SERAPI search results and returns a list of (title, snippet) pairs.

  Args:
      results: A dictionary containing the SERAPI search results.

  Returns:
      A list of (title, snippet) pairs.
  """
  entries = []
  for result in results.get('organic_results', []):
    title = result.get('title')
    snippet = result.get('snippet')
    link = result.get('link')
    if title and snippet and link:
      entries.append((title, snippet, link ))
  return entries

class FinSentioRAG:
    def __init__(self, collection_name, chroma_path, model_name="paraphrase-multilingual-mpnet-base-v2"):
        self.collection_name = collection_name
        self.chroma_path = chroma_path
        self.model_name = model_name
        self.current_pdf_path = None
        self.current_collection = None
        self.cached_pdf = {}  # PDF Ã¶nbelleÄŸi iÃ§in
        self.cached_chunks = {}  # Chunk Ã¶nbelleÄŸi iÃ§in
        self.client = PersistentClient(path=chroma_path, settings=Settings())
        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=model_name)
        self.sentence_transformer = SentenceTransformer(model_name)
        self.temp_collections = set()
        self.chunk_id_counter = 0  # Bu satÄ±rÄ± ekledik
        self.current_pdf_hash = None

    def create_temp_collection(self):
        try:
            unique_id = datetime.now().strftime("%Y%m%d_%H%M%S_") + str(uuid.uuid4())[:8]
            self.temp_collection_name = f"temp_{unique_id}"
            
            self.current_collection = self.client.get_or_create_collection(
                name=self.temp_collection_name,
                embedding_function=self.embedding_function
            )
            
            return self.temp_collection_name
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            raise

    def load_pdf(self, pdf_path, category="FinancialDocument"):
        try:
            # PDF hash'ini oluÅŸtur
            pdf_hash = self._get_file_hash(pdf_path)
            
            # EÄŸer aynÄ± PDF zaten yÃ¼klÃ¼yse, mevcut collection'Ä± kullan
            if self.current_pdf_path == pdf_path:
                return True

            # EÄŸer PDF Ã¶nbellekte varsa, onu kullan
            if pdf_hash in self.cached_chunks:
                self.current_collection = self.cached_chunks[pdf_hash]
                self.current_pdf_path = pdf_path
                return True

            # Yeni PDF yÃ¼kleniyorsa
            collection_name = self.create_temp_collection()
            self.current_pdf_hash = pdf_hash
            self.current_pdf_path = pdf_path
            
            # PDF'i text'e dÃ¶nÃ¼ÅŸtÃ¼r
            texts = self.convert_pdf_to_text(pdf_path)
            
            if not texts or (len(texts) == 1 and "hata" in texts[0].lower()):
                return False
            
            # Chunk'larÄ± oluÅŸtur
            char_chunks = self.split_text(texts, chunk_size=2000, chunk_overlap=200)
            token_chunks = self.token_split(char_chunks, tokens_per_chunk=128)
            
            # Metadata ekle
            ids, metadatas = self.add_metadata(token_chunks, os.path.basename(pdf_path), category)
            
            # Chroma'ya ekle
            self.current_collection.add(ids=ids, metadatas=metadatas, documents=token_chunks)
            
            # Ã–nbelleÄŸe kaydet
            self.cached_chunks[pdf_hash] = self.current_collection
            
            return True
            
        except Exception as e:
            print(f"PDF yÃ¼kleme hatasÄ±: {str(e)}")
            return False

    def _get_file_hash(self, file_path):
        """Dosya hash'ini hesapla"""
        import hashlib
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()

    def split_text(self, texts, chunk_size=2000, chunk_overlap=300):
        """Daha bÃ¼yÃ¼k chunk'lar kullanarak metni bÃ¶l"""
        splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", ". ", " ", ""],
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        return splitter.split_text("\n\n".join(texts))

    def token_split(self, chunks, tokens_per_chunk=128, chunk_overlap=64):
        splitter = SentenceTransformersTokenTextSplitter(
            chunk_overlap=chunk_overlap,
            model_name=self.model_name,
            tokens_per_chunk=tokens_per_chunk
        )
        token_chunks = []
        for c in chunks:
            token_chunks += splitter.split_text(c)
        return token_chunks

    def add_metadata(self, chunks, filename, category):
        """Chunk'lara metadata ekle"""
        ids = []
        metadatas = []
        
        for chunk in chunks:
            # Benzersiz ID oluÅŸtur
            chunk_id = f"{filename}_{self.chunk_id_counter}"
            self.chunk_id_counter += 1  # SayaÃ§ artÄ±r
            
            # Metadata oluÅŸtur
            metadata = {
                "source": filename,
                "category": category,
                "chunk_id": chunk_id
            }
            
            ids.append(chunk_id)
            metadatas.append(metadata)
            
        return ids, metadatas

    def convert_pdf_to_text(self, pdf_path):
        try:
            # PDF dosyasÄ±nÄ±n gerÃ§ekten var olup olmadÄ±ÄŸÄ±nÄ± kontrol et
            if not os.path.exists(pdf_path):
                return ["PDF dosyasÄ± bulunamadÄ± veya okunamadÄ±."]
            
            # Dosya boyutunu kontrol et
            file_size = os.path.getsize(pdf_path)
            
            # Dosya iÃ§eriÄŸini binary olarak oku ve ilk birkaÃ§ byte'Ä± kontrol et
            with open(pdf_path, 'rb') as f:
                header = f.read(4)  # PDF dosyalarÄ± %PDF ile baÅŸlamalÄ±
                if header != b'%PDF':
                    return ["Bu dosya geÃ§erli bir PDF dosyasÄ± deÄŸil."]
            
            try:
                reader = PdfReader(pdf_path)
                
                # Metin Ã§Ä±karma dene
                texts = []
                for i, page in enumerate(reader.pages):
                    extracted_text = page.extract_text()
                    if extracted_text and extracted_text.strip():
                        texts.append(extracted_text.strip())
                
                if not texts:
                    return ["Bu PDF dosyasÄ± metin iÃ§ermiyor veya metni Ã§Ä±karmak mÃ¼mkÃ¼n deÄŸil."]
                
                return texts
            except Exception as pdf_error:
                # DetaylÄ± hatayÄ± log dosyasÄ±na yaz
                with open("pdf_error.log", "a", encoding="utf-8") as f:
                    f.write(f"\n--- {datetime.now()} PDF Okuma HatasÄ± ---\n")
                    f.write(f"PDF Yolu: {pdf_path}\n")
                    f.write(f"Hata: {str(pdf_error)}\n")
                return [f"PDF okuma hatasÄ±: {str(pdf_error)}"]
        except Exception as e:
            # DetaylÄ± hatayÄ± log dosyasÄ±na yaz
            with open("pdf_error.log", "a", encoding="utf-8") as f:
                import traceback
                f.write(f"\n--- {datetime.now()} PDF Ä°ÅŸleme Genel HatasÄ± ---\n")
                f.write(f"PDF Yolu: {pdf_path}\n")
                f.write(f"Hata: {str(e)}\n")
                f.write(traceback.format_exc())
            return ["PDF iÅŸleme sÄ±rasÄ±nda bir hata oluÅŸtu."]

    def query(self, user_query, n_results=10, only_text=True):  # n_results'Ä± azalttÄ±k
        if not self.current_collection:
            return []
            
        # Daha az sonuÃ§ dÃ¶ndÃ¼r (daha hÄ±zlÄ±)
        results = self.current_collection.query(
            query_texts=[user_query],
            include=["documents", "metadatas", "distances"],
            n_results=n_results
        )
        return results["documents"][0] if only_text else results

    def clear_current_pdf(self):
        """Sadece kullanÄ±cÄ± istediÄŸinde PDF'i temizle"""
        if self.current_pdf_path:
            # Ã–nbellekten kaldÄ±r
            pdf_hash = self._get_file_hash(self.current_pdf_path)
            if pdf_hash in self.cached_chunks:
                del self.cached_chunks[pdf_hash]
            
            # Collection'Ä± temizle
            if self.current_collection:
                try:
                    self.client.delete_collection(self.current_collection.name)
                except:
                    pass
            
            self.current_pdf_path = None
            self.current_collection = None
            

def format_answer_with_clickable_links(raw_answer):
    if "Sources:" in raw_answer:
        body, sources_raw = raw_answer.split("Sources:")
        links = [line.strip("-â€¢ ") for line in sources_raw.strip().splitlines() if line.strip()]
        html_links = "<br>".join([f'<a href="{url}" target="_blank">{url}</a>' for url in links])
        html_answer = f"<div style='font-family: sans-serif; line-height: 1.6'>{body.strip()}<br><br><b>Kaynaklar:</b><br>{html_links}</div>"
    else:
        html_answer = raw_answer
    return html_answer
rag_instance = FinSentioRAG(
                collection_name="FinSentioDocs",
                chroma_path=CHROMADB_PATH
            )
# Ana fonksiyon - Gradio chatbot iÃ§in kullanÄ±lacak
def compute_md5(path):
    with open(path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()
def generate_financial_response(prompt, pdf_file=None, history=None, clear_pdf=False):
    global rag_instance
    try:
        if history is None:
            history = []
        chat_history = []
        for h in history:
            if len(h) == 2:
                chat_history.append({"role": "user", "parts": [h[0]]})
                chat_history.append({"role": "model", "parts": [h[1]]})

        # PDF temizleme isteÄŸi varsa
        if pdf_file is None and rag_instance.current_pdf_path:
            rag_instance.clear_current_pdf()

        if clear_pdf and rag_instance:
            rag_instance.clear_current_pdf()
            return "PDF temizlendi."
        blk = danger_agent.generate_response(prompt).strip()
        if blk != "safe":
            return blk
        intent = "file_analysis" if pdf_file else intent_classifier.generate_response(prompt).strip()
        print("Intent:", intent)    

        if intent == "file_analysis" and pdf_file is not None:
            temp_path = "temp_upload.pdf"
            try:
                # PDF iÅŸleme
                pdf_saved = False
                
                # Daha hÄ±zlÄ± dosya kopyalama
                if hasattr(pdf_file, 'name') and os.path.exists(pdf_file.name):
                    _, ext = os.path.splitext(pdf_file.name)
                    if ext.lower() != '.pdf':
                        return f"âš ï¸ LÃ¼tfen PDF dosyasÄ± yÃ¼kleyin. YÃ¼klenen dosya formatÄ±: {ext}"
                    
                    import shutil
                    shutil.copy2(pdf_file.name, temp_path)
                    pdf_saved = True
                
                if not pdf_saved:
                    if hasattr(pdf_file, 'read'):
                        with open(temp_path, 'wb') as f:
                            f.write(pdf_file.read())
                            pdf_saved = True
                    elif isinstance(pdf_file, bytes):
                        with open(temp_path, 'wb') as f:
                            f.write(pdf_file)
                            pdf_saved = True
                
                # PDF formatÄ± kontrolÃ¼
                with open(temp_path, 'rb') as f:
                    if f.read(4) != b'%PDF':
                        os.remove(temp_path)
                        return "âš ï¸ YÃ¼klenen dosya geÃ§erli bir PDF formatÄ±nda deÄŸil."
                temp_hash = rag_instance._get_file_hash(temp_path)
                new_hash = compute_md5(temp_path)
                # RAG iÅŸlemi - sadece yeni PDF yÃ¼klendiÄŸinde iÅŸle
                if rag_instance.current_pdf_hash is None or rag_instance.current_pdf_hash != new_hash:
                # 4a) Ã–nce varsa eski veriyi sil
                    rag_instance.clear_current_pdf()
                    # 4b) Yeni PDF'i yÃ¼kle ve hash'i gÃ¼ncelle
                    success = rag_instance.load_pdf(temp_path, category="user_uploaded")
                    if not success:
                        return "âš ï¸ PDF iÅŸlenemedi."
                    rag_instance.current_pdf_hash = new_hash
                
                # Daha az metin kullan
                texts = rag_instance.convert_pdf_to_text(temp_path)
                if not texts:
                    return "âš ï¸ PDF dosyasÄ±ndan metin Ã§Ä±karÄ±lamadÄ±."
                
                # Ä°lk 3000 karakteri kontrol et
                rel_pdf = relevance_agent.generate_response(texts[0][:3000]).strip()
                if rel_pdf != "relevant":
                    return rel_pdf

                # Daha az sonuÃ§ kullan
                context = "\n".join(rag_instance.query(prompt, n_results=10, only_text=True))
                
                # Gemini yanÄ±tÄ±
                rag_chat_agent = genai.GenerativeModel(
                    "gemini-2.0-flash",
                    system_instruction=role_rag,
                    safety_settings=safety_config
                )
                
                full_context_prompt = f"[DOCUMENT EXCERPTS]\n{context}\n\n[USER QUESTION]\n{prompt}"
                
                if chat_history:
                    chat = rag_chat_agent.start_chat(history=chat_history)
                    response = chat.send_message(full_context_prompt)
                else:
                    response = rag_chat_agent.generate_content(full_context_prompt)
                
                return format_answer_with_clickable_links(response.text)
                
            except Exception as e:
                return f"âš ï¸ PDF iÅŸlenirken bir hata oluÅŸtu: {str(e)}"

        # Web search yapÄ±lacaksa
        elif intent == "web_search":
            rel = relevance_agent.generate_response(prompt).strip()
            if rel != "relevant":
                return rel
            keywords = agent_keyword_generator.generate_response(prompt)
            
            results = search_google(keywords)
            
            parsed = parse_search_results(results)
            
            top_results = parsed[:30]
            summary_input = {
                "query": prompt,
                "results": [
                    {"title": t, "snippet": s, "link": l}
                    for t, s, l in top_results
                ]
            }
            full_summary_prompt = f"Search Query: {summary_input['query']}\nSearch Results: {json.dumps(summary_input['results'], ensure_ascii=False)}"
            
            summarizer = genai.GenerativeModel(
                "gemini-2.0-flash",
                system_instruction=role_summarize,  
                safety_settings=safety_config
            )
            
            if chat_history:
                chat = summarizer.start_chat(history=chat_history)
                summary = chat.send_message(full_summary_prompt)
            else:
                summary = summarizer.generate_content(full_summary_prompt)

            if summary.prompt_feedback and summary.prompt_feedback.block_reason:
                return "Ãœretilen cevap gÃ¼venlik filtresine takÄ±ldÄ±."

            return format_answer_with_clickable_links(summary.text)

    except Exception as e:
        return f"[Error]: {str(e)}"

    return "Finansal bir soru sorabilir veya analiz iÃ§in bir PDF yÃ¼kleyebilirsiniz."

def clear_pdf_and_chat():
    return None, []  # PDF'i ve chat geÃ§miÅŸini temizle
